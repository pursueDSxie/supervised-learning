{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伯努利朴素贝叶斯\n",
    "以文字出没出现作为向量化准则(1为出现，0为未出现)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList=[['my','dog','has','flea','problems','help','please'],\n",
    "            ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "            ['my','dalmation','is','so','cute','I','love','him'],\n",
    "            ['stop','posting','stupid','worthless','garbage'],\n",
    "            ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "            ['quit','buying','worthless','dog','food','stupid']]\n",
    "#六行文本，每一行对应一个分类(1代表侮辱性文字,0代表正常言论)\n",
    "classvec=[0,1,0,1,0,1]\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\">实现步骤</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.把所有出现的文字unique化，做成一个列表(可以把每个文字作为一个特征来看)\n",
    "\n",
    "2.把每行文本对应这所有特征进行向量化，即出现为1，未出现为0\n",
    "\n",
    "3.<span class=\"mark\">计算每个类别下每个文字数目占该类别下总文字数目的比例(条件概率)</span>。再计算每个类别的出现的概率。\n",
    "\n",
    "4.进行预测测试集，即把输入的文本中，每个文字出现概率相乘，但是为了避免概率为0(有些文字概率为0)，<span class=\"mark\">所以需要调整下初始文字的数目为1,总数目增加2.</span>\n",
    "\n",
    "同时由于概率相乘概率过小，所以将<span class=\"mark\">第3步骤中的条件概率计算取</span>$In$,从而使数值不那么小，对比不同类别下的概率，选择大的概率作为分类标准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\">注意：使用拉普拉斯平滑解决零概率问题；对乘积结果取自然对数避免下溢出问题，采用自然对数进行处理不会有任何损失。</span>条件概率一直相互乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得出现过的词语\n",
    "def uniqueword(words):\n",
    "    wordfeatures=set([])#使得加入的元素不会重复\n",
    "    for i in words:\n",
    "        wordfeatures=wordfeatures | set(i)#set(i)让加入的每行文本都是去重的,并集|作用是让行与行之间是去重的\n",
    "    return list(wordfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把文字转化为向量(即1为出现，0为未出现)\n",
    "def wordvec(wordfeatures,row_word):\n",
    "    p=np.zeros(len(wordfeatures))#初始化所有元素，都是未出现的\n",
    "    for unit_word in row_word:\n",
    "        if unit_word in wordfeatures:\n",
    "            p[wordfeatures.index(unit_word)]=1#把出现过的词语变为1\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算wordfeatures属于该类别的条件概率(用次数来做除法计算)和类别概率\n",
    "def prob(words,category,wordfeatures,wordvec):\n",
    "    p1=sum(category)/len(category)#得到侮辱性文字类别的概率\n",
    "    #初始化\n",
    "    p0w=np.ones(len(wordfeatures))#每个单词出现的数量为1，避免后续连乘时概率为0\n",
    "    p0sum=2#种类0的文字总数,初始化\n",
    "    p1w=np.ones(len(wordfeatures))\n",
    "    p1sum=2#种类1的文字总数,初始化\n",
    "    \n",
    "    for i in range(len(category)):\n",
    "        if category[i]==0:\n",
    "            p0sum+=sum(wordvec[i])#存在这一类别，且出现了,sum,并且是词向量\n",
    "            p0w+=wordvec[i]\n",
    "        else:\n",
    "            p1sum+=sum(wordvec[i])#存在这一类别，且出现了,sum,并且是词向量\n",
    "            p1w+=wordvec[i]\n",
    "    return np.log(p0w/p0sum),np.log(p1w/p1sum),p1#要取对数，避免值太小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.87180218 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.15948425 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -3.25809654 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936 -3.25809654\n",
      " -3.25809654 -3.25809654]\n"
     ]
    }
   ],
   "source": [
    "wordfeatures=uniqueword(postingList)#获得出现的词语\n",
    "#向量化所有文本\n",
    "wordvecs=[]\n",
    "for i in postingList:\n",
    "    wordvecs.append(wordvec(wordfeatures,i))\n",
    "#再次调用prob函数\n",
    "p0word,p1word,p1=prob(postingList,classvec,wordfeatures,wordvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立伯努利朴素贝叶斯分类器\n",
    "并进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立朴素贝叶斯分类器\n",
    "def BernoulliNB(test_vec,p0word,p1word,pclass1):\n",
    "    p0=sum(test_vec*p0word)+np.log(pclass1)\n",
    "    p1=sum(test_vec*p1word)+np.log(1-pclass1)\n",
    "    print(sum(test_vec*p0word))\n",
    "    if p0>p1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.436751534363128\n",
      "该文本属于0类别\n"
     ]
    }
   ],
   "source": [
    "test1=['love','my','dalmtion']\n",
    "\n",
    "print(f'该文本属于{BernoulliNB(wordvec(wordfeatures,test1),p0word,p1word,p1)}类别')#先把测试文本向量化，再传入分类器中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.516193076042964\n",
      "该文本属于1类别\n"
     ]
    }
   ],
   "source": [
    "test2=['stupid','garbage']\n",
    "\n",
    "print(f'该文本属于{BernoulliNB(wordvec(wordfeatures,test2),p0word,p1word,p1)}类别')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.]),\n",
       " array([1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]),\n",
       " array([1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=wordvec(uniqueword(postingList),['stupid','garbage'])\n",
    "wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=MultinomialNB()\n",
    "model.fit(np.array(wordvecs),np.array(classvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(a.reshape(1,-1))#预测种类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
